# nvidia-ai-infrastructure

## Introdu√ß√£o √† Infraestrutura de IA da NVIDIA (NCP-AI)

O design de infraestrutura de IA representa a base essencial para suportar cargas de trabalho de intelig√™ncia artificial em escala empresarial. Esta disciplina envolve a cria√ß√£o de ambientes computacionais especializados que integram hardware, software e recursos de rede otimizados especificamente para o ciclo de vida completo de projetos de ML e DL. Um design adequado precisa considerar desde o provisionamento de recursos acelerados para treinamento de modelos at√© a infraestrutura de infer√™ncia para deployment em produ√ß√£o, garantindo escalabilidade, confiabilidade e efici√™ncia de custos ao longo de todo o processo.

**Pontos importantes complementares:**
- Composta por 5 camadas essenciais: computa√ß√£o, armazenamento, rede, software stack e orquestra√ß√£o
- **Computa√ß√£o**: inclui GPUs, CPUs e DPUs para offloading de tarefas como seguran√ßa e networking
- **Armazenamento**: NVMe para velocidade local e object stores para escalabilidade
- **Networking**: tecnologias cr√≠ticas como NVLink e InfiniBand para mover grandes datasets
- **Orquestra√ß√£o**: Kubernetes √© especialmente popular em infraestruturas de IA
- Cargas de trabalho s√£o altamente din√¢micas - alguns jobs duram minutos, outros dias
- Princ√≠pios de design: escalabilidade, alta utiliza√ß√£o de recursos, flexibilidade, seguran√ßa e monitoriza√ß√£o

### Papel das GPUs em Cargas de Trabalho de IA
As GPUs emergiram como componentes cr√≠ticos para IA devido √† sua arquitetura massivamente paralela, ideal para processar as opera√ß√µes matriciais e de √°lgebra linear que fundamentam os algoritmos de deep learning. Diferente das CPUs com poucos n√∫cleos otimizados para tarefas sequenciais, as GPUs possuem milhares de n√∫cleos menores que processam simultaneamente grandes volumes de dados, reduzindo drasticamente o tempo necess√°rio para treinar modelos complexos. Esta capacidade de acelera√ß√£o paralela torna as GPUs indispens√°veis para redes neurais profundas, processamento de linguagem natural e vis√£o computacional.

**Pontos importantes complementares:**
- CPUs s√£o limitadas para IA devido ao baixo n√∫mero de n√∫cleos e modelo de execu√ß√£o serial
- GPUs seguem arquitetura SIMD (Single Instruction Multiple Data) - ideal para processamento paralelo
- Mem√≥ria de alta banda (HBM2, GDDR6) minimiza delays entre computa√ß√£o e acesso √† mem√≥ria
- **Benchmarks reais**: BERT em GPU √© 20x mais r√°pido que CPU; infer√™ncia tem at√© 70% menor lat√™ncia
- **ROI**: GPUs terminam jobs mais r√°pido, custo por modelo pode ser menor apesar do custo hor√°rio maior
- Frameworks como TensorFlow e PyTorch oferecem suporte nativo GPU via CUDA e cuDNN

### Arquiteturas CPU vs GPU vs DPU
As CPUs, GPUs e DPUs representam tr√™s paradigmas distintos de processamento complementares em infraestruturas de IA modernas. As CPUs funcionam como c√©rebro geral do sistema, gerenciando controle, l√≥gica e tarefas sequenciais. As GPUs atuam como aceleradores especializados para computa√ß√£o paralela massiva em dados. J√° as DPUs (Data Processing Units) s√£o processadores inteligentes que descarregam tarefas de infraestrutura como networking, armazenamento e seguran√ßa, liberando CPUs e GPUs para focarem em cargas de trabalho de aplica√ß√£o. Esta tr√≠ade forma a base da computa√ß√£o heterog√™nea contempor√¢nea.

**Pontos importantes complementares:**
- **CPU**: "c√©rebro central" - executa OS, gerencia mem√≥ria, orquestra jobs entre GPUs/TPUs
- **GPU**: "cavalos de for√ßa" da IA - milhares de n√∫cleos leves para opera√ß√µes paralelas
- **DPU**: novo player - purpose-built para networking, storage, IO, seguran√ßa e telemetria
- DPUs permitem compartilhamento seguro de clusters GPU entre m√∫ltiplos usu√°rios
- **Modelo dos tr√™s chips da NVIDIA**: CPU (orquestra√ß√£o), GPU (computa√ß√£o), DPU (infraestrutura)
- **Exemplo real**: Bluefield DPU combina 400Gb networking com acelera√ß√£o criptogr√°fica

### Acelera√ß√£o por GPU para Pipelines de IA/ML
A acelera√ß√£o por GPU revolucionou os pipelines de IA/ML ao otimizar cada etapa do fluxo de trabalho. Desde o pr√©-processamento de dados em larga escala at√© o treinamento iterativo de modelos e a infer√™ncia de alta throughput, as GPUs proporcionam ganhos de performance orders de magnitude superiores √†s solu√ß√µes baseadas apenas em CPU. Esta acelera√ß√£o permite experimenta√ß√£o mais r√°pida, redu√ß√£o do time-to-market e capacidade de lidar com datasets e modelos cada vez maiores e mais complexos, tornando vi√°veis aplica√ß√µes que antes eram computacionalmente proibitivas.

**Pontos importantes complementares:**
- **Pr√©-processamento**: pode consumir 30-50% do tempo total do pipeline
- **RAPIDS**: fornece alternativas GPU para pandas, NumPy e scikit-learn (10-100x mais r√°pido)
- **DALI**: acelera pr√©-processamento de imagens/v√≠deo com opera√ß√µes no GPU
- **Treinamento distribu√≠do**: Horovod, PyTorch DDP permitem escala em m√∫ltiplas GPUs/m√°quinas
- **Hyperparameter tuning**: GPUs permitem execu√ß√£o paralela de centenas de training runs
- **Infer√™ncia**: TensorRT para otimiza√ß√£o + Triton para deployment escal√°vel
- Fluxo completo acelerado reduz time-to-market e overhead operacional

### Vis√£o Geral do Ecossistema NVIDIA (CUDA, Triton, NGC)
O ecossistema NVIDIA constitui uma plataforma abrangente e integrada para IA enterprise, centrada em tr√™s pilares principais: CUDA fornece o modelo de programa√ß√£o paralela que habilita a acelera√ß√£o por GPU; Triton Inference Server oferece um ambiente unificado para deployment de modelos em produ√ß√£o com suporte a m√∫ltiplos frameworks e otimiza√ß√µes de performance; e o NGC (NVIDIA GPU Cloud) funciona como um cat√°logo de softwares, modelos pr√©-treinados e containers otimizados que aceleram o desenvolvimento e a implanta√ß√£o de solu√ß√µes de IA. Juntos, esses componentes formam um stack coeso que simplifica a constru√ß√£o e opera√ß√£o de infraestruturas de IA escal√°veis.

**Pontos importantes complementares:**
- **Stack completo**: drivers, bibliotecas, modelos pr√©-treinados, containers e ferramentas de orquestra√ß√£o
- **CUDA**: plataforma paralela que suporta C++, Python, Fortran - base de tudo
- **cuDNN**: opera√ß√µes de baixo n√≠vel para redes neurais (convolu√ß√µes, pooling, ativa√ß√µes)
- **NGC**: hub central com Docker containers pr√©-configurados e modelos pr√©-treinados
- **Triton**: suporte multi-framework (TensorFlow, PyTorch, ONNX, XGBoost) com dynamic batching
- **DOCA**: SDK para programar DPUs - zero trust, multi-tenant isolation, observabilidade em tempo real
- Integra√ß√£o unificada permite portabilidade entre ambientes e performance m√°xima

## Gerenciamento de Recursos de GPU e Virtualiza√ß√£o

### Configura√ß√£o MIG (Multi-instance GPU)

A tecnologia **MIG (Multi-instance GPU)**, introduzida pela NVIDIA a partir da arquitetura Ampere (ex: GPUs A100, H100), permite particionar uma √∫nica GPU f√≠sica em v√°rias inst√¢ncias menores e totalmente isoladas no n√≠vel de hardware. Cada inst√¢ncia MIG possui seus pr√≥prios **Streaming Multiprocessors (SMs)**, **mem√≥ria dedicada** (ex: 1GB, 5GB, 10GB) e **cache L2** alocados de forma fixa, operando de maneira independente e segura, como se fossem GPUs separadas. Isso √© fundamental para **maximizar a utiliza√ß√£o** em ambientes de *multi-tenancy* (como clouds, JupyterHub ou plataformas de infer√™ncia), onde m√∫ltiplos usu√°rios ou cargas de trabalho (ex: diferentes modelos de IA) podem ser executados em paralelo na mesma GPU, sem risco de conten√ß√£o de recursos ou interfer√™ncia ("*noisy neighbors*"). A configura√ß√£o √© feita via comandos `nvidia-smi` (ex: `nvidia-smi mig -cgi 1g.5gb`) e pode ser gerenciada no Kubernetes via *device plugin* espec√≠fico, permitindo aloca√ß√£o granular e previs√≠vel. O MIG √© ideal para cen√°rios que exigem **isolamento rigoroso** e efici√™ncia de custos, transformando uma GPU poderosa (e muitas vezes subutilizada) em v√°rios recursos menores e dedicados.

### T√©cnicas de Partilha e Isolamento de GPU

Para ambientes que n√£o suportam MIG (GPUs mais antigas) ou necessitam de partilha mais din√¢mica, existem t√©cnicas de software para **partilha e isolamento de GPU**. A mais comum √© o **Time-Slicing**, onde a GPU alterna entre processos em intervalos de tempo, sem isolamento f√≠sico ‚Äì pr√°tico para desenvolvimento ou infer√™ncia leve, mas sujeito a imprevisibilidade de desempenho. J√° a **isola√ß√£o via cont√™ineres** (Docker/ Kubernetes) com o **NVIDIA Container Toolkit** permite atribuir GPUs espec√≠ficas a cont√™ineres, usando *cgroups* para limitar recursos e evitar conflitos. Frameworks como TensorFlow e PyTorch oferecem controles de mem√≥ria (ex: `tf.config.experimental.set_memory_growth`) para aloca√ß√£o consciente. No Kubernetes, o **NVIDIA Device Plugin** √© essencial, expondo GPUs como recursos program√°veis e permitindo o uso de *taints/tolerations* e *resource quotas* para agendamento justo. A escolha da t√©cnica envolve trade-offs: Time-Slicing √© simples mas menos isolado; cont√™ineres oferecem l√≥gica de isolamento; MIG garante separa√ß√£o f√≠sica (mas requer hardware espec√≠fico). A monitoriza√ß√£o cont√≠nua com `nvidia-smi` ou **DCGM** √© crucial para evitar satura√ß√£o de mem√≥ria e garantir *fairness*.

### Configura√ß√£o e Casos de Uso de GPUs Virtuais (vGPU)

**GPUs Virtuais (vGPU)** da NVIDIA permitem virtualizar uma GPU f√≠sica para ser partilhada por m√∫ltiplas **M√°quinas Virtuais (VMs)**, cada uma recebendo uma fatia dedicada de computa√ß√£o e mem√≥ria atrav√©s do *hypervisor* (ex: VMware vSphere, Citrix Hypervisor, KVM). Diferente do MIG (focado em bare-metal/cont√™ineres), o vGPUs √© voltado para ambientes **virtualizados tradicionais**, sendo essencial para **VDI (Virtual Desktop Infrastructure)** ‚Äì onde utilizadores remotos acedem a desktops com acelera√ß√£o gr√°fica ‚Äì e para infraestruturas de cloud que oferecem inst√¢ncias de VM com GPU. S√£o definidos **perfis** (ex: vComputeServer para cargas computacionais, Quadro Virtual para gr√°ficos) que determinam a quantidade de recursos alocados por VM. A configura√ß√£o exige licen√ßas espec√≠ficas da NVIDIA, *drivers* compat√≠veis no host e nas VMs, e software de gest√£o (NVIDIA vGPU Software). Os casos de uso abrangem desde **esta√ß√µes de trabalho virtuais** para engenheiros de IA e designers gr√°ficos at√© ambientes de **multi-inquilinato seguros** em universidades ou empresas, combinando a flexibilidade da virtualiza√ß√£o (snapshots, migra√ß√£o) com o poder de processamento acelerado.

### Agendamento de Cargas de Trabalho com GPU no Kubernetes

O Kubernetes tornou-se a plataforma padr√£o para orquestrar cargas de trabalho aceleradas por GPU em escala. Para isso, √© necess√°rio instalar os **drivers NVIDIA** nos *nodes* e implantar o **NVIDIA Device Plugin** como um *DaemonSet*, que permite ao Kubernetes detetar e gerir GPUs como recursos program√°veis (semelhante a CPU/mem√≥ria). Nos *manifests* dos *Pods*, especifica-se o recurso `nvidia.com/gpu` sob `requests` e `limits` para garantir agendamento exclusivo. T√©cnicas avan√ßadas como **nodeAffinity**, **taints/tolerations** e **resource quotas** ajudam a isolar *nodes* com GPU e a distribuir cargas de forma justa entre utilizadores ou equipas. Para GPUs com **MIG** (ex: A100), o *device plugin* suporta a exposi√ß√£o de inst√¢ncias individuais (ex: 1g.5gb) como recursos distintos, permitindo agendamento granular e multi-inquilinato seguro. A monitoriza√ß√£o √© feita com ferramentas como **NVIDIA DCGM** integrado com Prometheus/Grafana, fornecendo m√©tricas detalhadas de utiliza√ß√£o por *pod*. Melhores pr√°ticas incluem evitar misturar treino e infer√™ncia no mesmo *node*, usar *PriorityClasses* para cargas cr√≠ticas e considerar *schedulers* avan√ßados (ex: Volcano) para *batch jobs*. Esta abordagem permite gerir clusters de GPU de forma eficiente, resiliente e escal√°vel.

### Laborat√≥rio Pr√°tico: Configurar MIG numa A100

Este laborat√≥rio pr√°tico oferece uma experi√™ncia hands-on para configurar a tecnologia **MIG numa GPU NVIDIA A100**. Os participantes aprender√£o a ativar o modo MIG via `nvidia-smi`, a criar e gerir diferentes **perfis de inst√¢ncia** (ex: 1g.5gb, 2g.10gb, 3g.20gb) que dividem a GPU em parti√ß√µes isoladas, e a atribuir essas inst√¢ncias a cont√™ineres ou cargas de trabalho espec√≠ficas. O exerc√≠cio inclui a verifica√ß√£o da configura√ß√£o com comandos como `nvidia-smi mig -l` e a explora√ß√£o de cen√°rios reais, como a execu√ß√£o paralela de m√∫ltiplos modelos de infer√™ncia ou ambientes de desenvolvimento isolados na mesma GPU f√≠sica. Este laborat√≥rio √© essencial para compreender na pr√°tica como implementar **multi-inquilinato seguro e eficiente**, maximizando o retorno do investimento em hardware de √∫ltima gera√ß√£o e preparando a infraestrutura para ambientes de produ√ß√£o escal√°veis.

#### üìã Pr√©-requisitos

- Sistema com **NVIDIA A100 GPU**
- Ubuntu 20.04+ (bare metal ou VM)
- Docker & Kubernetes (v1.20+)
- nvidia-container-toolkit, nvidia-docker2
- NVIDIA GPU Driver (465+)
- nvidia-smi, kubectl, helm

#### üöÄ Execu√ß√£o do Laborat√≥rio

Execute os scripts na ordem:

```bash
# 1. Habilitar modo MIG
chmod +x scripts/1-enable-mig.sh
./scripts/1-enable-mig.sh

# 2. Criar inst√¢ncias MIG
chmod +x scripts/2-create-mig-instances.sh
./scripts/2-create-mig-instances.sh

# 3. Implantar device plugin no Kubernetes
chmod +x scripts/3-deploy-device-plugin.sh
./scripts/3-deploy-device-plugin.sh

# 4. Testar com pod de exemplo
chmod +x scripts/4-deploy-test-pod.sh
./scripts/4-deploy-test-pod.sh

# Para monitoramento 
kubectl apply -f manifests/dcgm-exporter.yaml

```

As Inst√¢ncias MIG n√£o s√£o persistentes ap√≥s reboot (use systemd para automa√ß√£o). Ajuste nodeSelector no mig-pod.yaml conforme seu ambiente e o comando ``nvidia-smi mig -lgip`` para ver perfis dispon√≠veis.

## Armazenamento, Redes e Pipelines de Dados para IA

### Arquiteturas de Armazenamento para Cargas de Trabalho de IA (local, compartilhado, objeto)

O armazenamento na infraestrutura de IA √© um componente cr√≠tico de desempenho, pois modelos de grande escala realizam leituras e escritas massivas de dados. Escolher a arquitetura correta ‚Äî desde SSDs locais (NVMe) para velocidade, armazenamento compartilhado (como NFS ou Lustre) para treinamento distribu√≠do, ou armazenamento de objeto (S3, GCS) para escalabilidade e custo ‚Äî impacta diretamente a throughput, lat√™ncia e utiliza√ß√£o da GPU. Sistemas do mundo real frequentemente combinam esses tipos em uma arquitetura h√≠brida e em camadas (hot, warm, cold data) para otimizar custo e performance, garantindo que os GPUs nunca fiquem ociosos esperando por dados.

### Rede de Alta Velocidade: NVLink, Infiniband, RDMA

Em cargas de trabalho de IA distribu√≠da, a rede √© t√£o crucial quanto o poder de computa√ß√£o. Tecnologias como o NVLink da NVIDIA permitem comunica√ß√£o ultrarr√°pida entre GPUs no mesmo n√≥, enquanto o InfiniBand √© o padr√£o-ouro para interconex√£o de alta largura de banda e baixa lat√™ncia entre n√≥s em clusters. O RDMA (Remote Direct Memory Access) √© fundamental, permitindo a transfer√™ncia direta de dados entre a mem√≥ria de m√°quinas diferentes, contornando a CPU e reduzindo drasticamente a lat√™ncia e a sobrecarga. A combina√ß√£o dessas tecnologias, juntamente com features como GPUDirect, √© essencial para opera√ß√µes como "all-reduce" durante o treinamento distribu√≠do de modelos grandes, como GPT ou BERT, garantindo que a sincroniza√ß√£o de gradientes n√£o se torne um gargalo.

### Gargalos e Otimiza√ß√£o no Movimento de Dados

Um gargalo no movimento de dados ‚Äî seja em E/S de disco, na rede ou no pr√©-processamento ‚Äî pode deixar GPUs caros ociosas, aumentando o tempo de treinamento e custos operacionais. Identificar esses pontos √© o primeiro passo, utilizando ferramentas como \texttt{iostat}, \texttt{nvtop} ou profilers de framework (TensorFlow, PyTorch). A otimiza√ß√£o envolve estrat√©gias como a ado√ß√£o de NVMe, uso de carregamento de dados multi-thread (ex: \texttt{num\_workers} no PyTorch), prefetching, caching de dados localmente e paraleliza√ß√£o do pr√©-processamento (ex: com NVIDIA DALI). Em n√≠vel de rede, tuning de configura√ß√µes (MTU, buffers) e a ado√ß√£o de InfiniBand com RDMA s√£o chave para um fluxo de dados cont√≠nuo e eficiente do storage at√© a GPU.

### Design de Pipeline de Dados para IA (ETL + Treinamento + Infer√™ncia)

Um pipeline de dados de IA bem projetado √© um sistema interconectado que abrange desde a ingest√£o de dados brutos (ETL) at√© o treinamento e a infer√™ncia. O est√°gio de ETL, frequentemente orquestrado por ferramentas como Apache Airflow e acelerado por GPUs (RAPIDS, DALI), √© respons√°vel por extrair, transformar e carregar dados em um storage acess√≠vel. No treinamento, o pipeline deve alimentar os GPUs de forma cont√≠nua, usando data loaders paralelizados e formatos eficientes. Para infer√™ncia, em lote ou tempo real, servidores de modelo como o Triton Inference Server s√£o utilizados para oferecer baixa lat√™ncia e alto throughput. Projetar com resili√™ncia, monitoramento e est√°gios desacoplados (usando filas como Kafka) garante um pipeline robusto e escal√°vel. 

### Laborat√≥rio: Projetar um Pipeline de Dados de Ponta a Ponta para IA

Neste laborat√≥rio pr√°tico, consolidamos todos os conceitos anteriores para projetar e implementar um pipeline completo. Isso envolve a configura√ß√£o de uma arquitetura de armazenamento em camadas (ex: S3 para dados brutos, BeeGFS/Lustre para datasets de treinamento), a configura√ß√£o de rede de alta velocidade (InfiniBand com RDMA) e a constru√ß√£o do fluxo de dados em si. Voc√™ poder√° orquestrar um pipeline que ingere dados de um stream em tempo real (Kafka), realiza ETL acelerada, treina um modelo em um cluster de GPUs interconectados com NVLink/InfiniBand e, finalmente, implanta o modelo para infer√™ncia em um ambiente escal√°vel como Kubernetes, utilizando otimiza√ß√µes para evitar gargalos e garantir a m√°xima utiliza√ß√£o dos recursos.

#### üéØ Objetivo

Criar um pipeline completo de IA demonstrando:
- **ETL** com NVIDIA DALI
- **Treinamento** com PyTorch
- **Inference** com Triton Server
- **Otimiza√ß√£o** com GPU

#### üõ†Ô∏è Pr√©-requisitos

- NVIDIA GPU (A100, V100, RTX 3090, etc)
- Docker e NVIDIA Container Toolkit
- Python 3.8+

#### ‚ö° Configura√ß√£o R√°pida

```bash
# 1. Clonar reposit√≥rio
git clone https://github.com/seu-usuario/lab-ai-pipeline.git
cd lab-ai-pipeline

# 2. Executar setup autom√°tico
chmod +x scripts/setup_environment.sh
./scripts/setup_environment.sh

# 3. Ativar ambiente virtual
source venv/bin/activate

```

Data Source ‚Üí ETL (DALI) ‚Üí Training (PyTorch) ‚Üí Model ‚Üí Triton Server ‚Üí Inference Client

Treinar o modelo:

```
python src/train_model.py
```

Iniciar o servidor triton:

```bash
# Usando Docker Compose (recomendado)
docker-compose up triton-server
```

```bash
# Ou manualmente
docker run --gpus all -p8000:8000 -p8001:8001 -p8002:8002 \
  -v$(pwd)/model_repository:/models \
  nvcr.io/nvidia/tritonserver:24.03-py3 \
  tritonserver --model-repository=/models
```

Testar inferencia:

```bash
python src/inference_client.py
```

Jupyternotebook:
```bash
docker-compose up jupyter
# Acesse: http://localhost:8888
```

Benchmark:

```bash
# Instalar perf_analyzer
docker exec -it <triton_container> perf_analyzer -m my_model -b 8 -u localhost:8000
```

Monitoramento: 

```bash
watch -n 1 nvidia-smi
```

####Vers√£o Simplificada

Pipeline completo ETL ‚Üí Treinamento ‚Üí Inference em 3 arquivos!

## ‚ö° Comece Agora

```bash
# 1. Clone e instale
git clone <seu-repositorio>
cd lab-ai-pipeline
pip install -r requirements.txt

# 2. Treine o modelo
python train.py

# 3. Inicie o servidor (terminal 1)
docker run --gpus all -p8000:8000 -p8001:8001 -p8002:8002 -v$(pwd)/model_repository:/models nvcr.io/nvidia/tritonserver:24.03-py3 tritonserver --model-repository=/models

# 4. Teste inference (terminal 2)
python inference.py
```

## Orquestra√ß√£o e Escalabilidade de Clusters de IA

## Otimiza√ß√£o de Desempenho e Monitoramento

## Seguran√ßa, Conformidade e Governan√ßa de Dados

## Infraestrutura de IA na Edge e Integra√ß√£o

## NGC, Triton Inference Server e Implanta√ß√£o

## Projetos do Mundo Real e Fluxos de Trabalho Empresariais

## Projeto Final e Prepara√ß√£o para Certifica√ß√£o
