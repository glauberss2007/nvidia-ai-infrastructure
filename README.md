# nvidia-ai-infrastructure

## Introdu√ß√£o √† Infraestrutura de IA da NVIDIA (NCP-AI)

O design de infraestrutura de IA representa a base essencial para suportar cargas de trabalho de intelig√™ncia artificial em escala empresarial. Esta disciplina envolve a cria√ß√£o de ambientes computacionais especializados que integram hardware, software e recursos de rede otimizados especificamente para o ciclo de vida completo de projetos de ML e DL. Um design adequado precisa considerar desde o provisionamento de recursos acelerados para treinamento de modelos at√© a infraestrutura de infer√™ncia para deployment em produ√ß√£o, garantindo escalabilidade, confiabilidade e efici√™ncia de custos ao longo de todo o processo.

**Pontos importantes complementares:**
- Composta por 5 camadas essenciais: computa√ß√£o, armazenamento, rede, software stack e orquestra√ß√£o
- **Computa√ß√£o**: inclui GPUs, CPUs e DPUs para offloading de tarefas como seguran√ßa e networking
- **Armazenamento**: NVMe para velocidade local e object stores para escalabilidade
- **Networking**: tecnologias cr√≠ticas como NVLink e InfiniBand para mover grandes datasets
- **Orquestra√ß√£o**: Kubernetes √© especialmente popular em infraestruturas de IA
- Cargas de trabalho s√£o altamente din√¢micas - alguns jobs duram minutos, outros dias
- Princ√≠pios de design: escalabilidade, alta utiliza√ß√£o de recursos, flexibilidade, seguran√ßa e monitoriza√ß√£o

### Papel das GPUs em Cargas de Trabalho de IA
As GPUs emergiram como componentes cr√≠ticos para IA devido √† sua arquitetura massivamente paralela, ideal para processar as opera√ß√µes matriciais e de √°lgebra linear que fundamentam os algoritmos de deep learning. Diferente das CPUs com poucos n√∫cleos otimizados para tarefas sequenciais, as GPUs possuem milhares de n√∫cleos menores que processam simultaneamente grandes volumes de dados, reduzindo drasticamente o tempo necess√°rio para treinar modelos complexos. Esta capacidade de acelera√ß√£o paralela torna as GPUs indispens√°veis para redes neurais profundas, processamento de linguagem natural e vis√£o computacional.

**Pontos importantes complementares:**
- CPUs s√£o limitadas para IA devido ao baixo n√∫mero de n√∫cleos e modelo de execu√ß√£o serial
- GPUs seguem arquitetura SIMD (Single Instruction Multiple Data) - ideal para processamento paralelo
- Mem√≥ria de alta banda (HBM2, GDDR6) minimiza delays entre computa√ß√£o e acesso √† mem√≥ria
- **Benchmarks reais**: BERT em GPU √© 20x mais r√°pido que CPU; infer√™ncia tem at√© 70% menor lat√™ncia
- **ROI**: GPUs terminam jobs mais r√°pido, custo por modelo pode ser menor apesar do custo hor√°rio maior
- Frameworks como TensorFlow e PyTorch oferecem suporte nativo GPU via CUDA e cuDNN

### Arquiteturas CPU vs GPU vs DPU
As CPUs, GPUs e DPUs representam tr√™s paradigmas distintos de processamento complementares em infraestruturas de IA modernas. As CPUs funcionam como c√©rebro geral do sistema, gerenciando controle, l√≥gica e tarefas sequenciais. As GPUs atuam como aceleradores especializados para computa√ß√£o paralela massiva em dados. J√° as DPUs (Data Processing Units) s√£o processadores inteligentes que descarregam tarefas de infraestrutura como networking, armazenamento e seguran√ßa, liberando CPUs e GPUs para focarem em cargas de trabalho de aplica√ß√£o. Esta tr√≠ade forma a base da computa√ß√£o heterog√™nea contempor√¢nea.

**Pontos importantes complementares:**
- **CPU**: "c√©rebro central" - executa OS, gerencia mem√≥ria, orquestra jobs entre GPUs/TPUs
- **GPU**: "cavalos de for√ßa" da IA - milhares de n√∫cleos leves para opera√ß√µes paralelas
- **DPU**: novo player - purpose-built para networking, storage, IO, seguran√ßa e telemetria
- DPUs permitem compartilhamento seguro de clusters GPU entre m√∫ltiplos usu√°rios
- **Modelo dos tr√™s chips da NVIDIA**: CPU (orquestra√ß√£o), GPU (computa√ß√£o), DPU (infraestrutura)
- **Exemplo real**: Bluefield DPU combina 400Gb networking com acelera√ß√£o criptogr√°fica

### Acelera√ß√£o por GPU para Pipelines de IA/ML
A acelera√ß√£o por GPU revolucionou os pipelines de IA/ML ao otimizar cada etapa do fluxo de trabalho. Desde o pr√©-processamento de dados em larga escala at√© o treinamento iterativo de modelos e a infer√™ncia de alta throughput, as GPUs proporcionam ganhos de performance orders de magnitude superiores √†s solu√ß√µes baseadas apenas em CPU. Esta acelera√ß√£o permite experimenta√ß√£o mais r√°pida, redu√ß√£o do time-to-market e capacidade de lidar com datasets e modelos cada vez maiores e mais complexos, tornando vi√°veis aplica√ß√µes que antes eram computacionalmente proibitivas.

**Pontos importantes complementares:**
- **Pr√©-processamento**: pode consumir 30-50% do tempo total do pipeline
- **RAPIDS**: fornece alternativas GPU para pandas, NumPy e scikit-learn (10-100x mais r√°pido)
- **DALI**: acelera pr√©-processamento de imagens/v√≠deo com opera√ß√µes no GPU
- **Treinamento distribu√≠do**: Horovod, PyTorch DDP permitem escala em m√∫ltiplas GPUs/m√°quinas
- **Hyperparameter tuning**: GPUs permitem execu√ß√£o paralela de centenas de training runs
- **Infer√™ncia**: TensorRT para otimiza√ß√£o + Triton para deployment escal√°vel
- Fluxo completo acelerado reduz time-to-market e overhead operacional

### Vis√£o Geral do Ecossistema NVIDIA (CUDA, Triton, NGC)
O ecossistema NVIDIA constitui uma plataforma abrangente e integrada para IA enterprise, centrada em tr√™s pilares principais: CUDA fornece o modelo de programa√ß√£o paralela que habilita a acelera√ß√£o por GPU; Triton Inference Server oferece um ambiente unificado para deployment de modelos em produ√ß√£o com suporte a m√∫ltiplos frameworks e otimiza√ß√µes de performance; e o NGC (NVIDIA GPU Cloud) funciona como um cat√°logo de softwares, modelos pr√©-treinados e containers otimizados que aceleram o desenvolvimento e a implanta√ß√£o de solu√ß√µes de IA. Juntos, esses componentes formam um stack coeso que simplifica a constru√ß√£o e opera√ß√£o de infraestruturas de IA escal√°veis.

**Pontos importantes complementares:**
- **Stack completo**: drivers, bibliotecas, modelos pr√©-treinados, containers e ferramentas de orquestra√ß√£o
- **CUDA**: plataforma paralela que suporta C++, Python, Fortran - base de tudo
- **cuDNN**: opera√ß√µes de baixo n√≠vel para redes neurais (convolu√ß√µes, pooling, ativa√ß√µes)
- **NGC**: hub central com Docker containers pr√©-configurados e modelos pr√©-treinados
- **Triton**: suporte multi-framework (TensorFlow, PyTorch, ONNX, XGBoost) com dynamic batching
- **DOCA**: SDK para programar DPUs - zero trust, multi-tenant isolation, observabilidade em tempo real
- Integra√ß√£o unificada permite portabilidade entre ambientes e performance m√°xima

## Gerenciamento de Recursos de GPU e Virtualiza√ß√£o

### Configura√ß√£o MIG (Multi-instance GPU)

A tecnologia **MIG (Multi-instance GPU)**, introduzida pela NVIDIA a partir da arquitetura Ampere (ex: GPUs A100, H100), permite particionar uma √∫nica GPU f√≠sica em v√°rias inst√¢ncias menores e totalmente isoladas no n√≠vel de hardware. Cada inst√¢ncia MIG possui seus pr√≥prios **Streaming Multiprocessors (SMs)**, **mem√≥ria dedicada** (ex: 1GB, 5GB, 10GB) e **cache L2** alocados de forma fixa, operando de maneira independente e segura, como se fossem GPUs separadas. Isso √© fundamental para **maximizar a utiliza√ß√£o** em ambientes de *multi-tenancy* (como clouds, JupyterHub ou plataformas de infer√™ncia), onde m√∫ltiplos usu√°rios ou cargas de trabalho (ex: diferentes modelos de IA) podem ser executados em paralelo na mesma GPU, sem risco de conten√ß√£o de recursos ou interfer√™ncia ("*noisy neighbors*"). A configura√ß√£o √© feita via comandos `nvidia-smi` (ex: `nvidia-smi mig -cgi 1g.5gb`) e pode ser gerenciada no Kubernetes via *device plugin* espec√≠fico, permitindo aloca√ß√£o granular e previs√≠vel. O MIG √© ideal para cen√°rios que exigem **isolamento rigoroso** e efici√™ncia de custos, transformando uma GPU poderosa (e muitas vezes subutilizada) em v√°rios recursos menores e dedicados.

### T√©cnicas de Partilha e Isolamento de GPU

Para ambientes que n√£o suportam MIG (GPUs mais antigas) ou necessitam de partilha mais din√¢mica, existem t√©cnicas de software para **partilha e isolamento de GPU**. A mais comum √© o **Time-Slicing**, onde a GPU alterna entre processos em intervalos de tempo, sem isolamento f√≠sico ‚Äì pr√°tico para desenvolvimento ou infer√™ncia leve, mas sujeito a imprevisibilidade de desempenho. J√° a **isola√ß√£o via cont√™ineres** (Docker/ Kubernetes) com o **NVIDIA Container Toolkit** permite atribuir GPUs espec√≠ficas a cont√™ineres, usando *cgroups* para limitar recursos e evitar conflitos. Frameworks como TensorFlow e PyTorch oferecem controles de mem√≥ria (ex: `tf.config.experimental.set_memory_growth`) para aloca√ß√£o consciente. No Kubernetes, o **NVIDIA Device Plugin** √© essencial, expondo GPUs como recursos program√°veis e permitindo o uso de *taints/tolerations* e *resource quotas* para agendamento justo. A escolha da t√©cnica envolve trade-offs: Time-Slicing √© simples mas menos isolado; cont√™ineres oferecem l√≥gica de isolamento; MIG garante separa√ß√£o f√≠sica (mas requer hardware espec√≠fico). A monitoriza√ß√£o cont√≠nua com `nvidia-smi` ou **DCGM** √© crucial para evitar satura√ß√£o de mem√≥ria e garantir *fairness*.

### Configura√ß√£o e Casos de Uso de GPUs Virtuais (vGPU)

**GPUs Virtuais (vGPU)** da NVIDIA permitem virtualizar uma GPU f√≠sica para ser partilhada por m√∫ltiplas **M√°quinas Virtuais (VMs)**, cada uma recebendo uma fatia dedicada de computa√ß√£o e mem√≥ria atrav√©s do *hypervisor* (ex: VMware vSphere, Citrix Hypervisor, KVM). Diferente do MIG (focado em bare-metal/cont√™ineres), o vGPUs √© voltado para ambientes **virtualizados tradicionais**, sendo essencial para **VDI (Virtual Desktop Infrastructure)** ‚Äì onde utilizadores remotos acedem a desktops com acelera√ß√£o gr√°fica ‚Äì e para infraestruturas de cloud que oferecem inst√¢ncias de VM com GPU. S√£o definidos **perfis** (ex: vComputeServer para cargas computacionais, Quadro Virtual para gr√°ficos) que determinam a quantidade de recursos alocados por VM. A configura√ß√£o exige licen√ßas espec√≠ficas da NVIDIA, *drivers* compat√≠veis no host e nas VMs, e software de gest√£o (NVIDIA vGPU Software). Os casos de uso abrangem desde **esta√ß√µes de trabalho virtuais** para engenheiros de IA e designers gr√°ficos at√© ambientes de **multi-inquilinato seguros** em universidades ou empresas, combinando a flexibilidade da virtualiza√ß√£o (snapshots, migra√ß√£o) com o poder de processamento acelerado.

### Agendamento de Cargas de Trabalho com GPU no Kubernetes

O Kubernetes tornou-se a plataforma padr√£o para orquestrar cargas de trabalho aceleradas por GPU em escala. Para isso, √© necess√°rio instalar os **drivers NVIDIA** nos *nodes* e implantar o **NVIDIA Device Plugin** como um *DaemonSet*, que permite ao Kubernetes detetar e gerir GPUs como recursos program√°veis (semelhante a CPU/mem√≥ria). Nos *manifests* dos *Pods*, especifica-se o recurso `nvidia.com/gpu` sob `requests` e `limits` para garantir agendamento exclusivo. T√©cnicas avan√ßadas como **nodeAffinity**, **taints/tolerations** e **resource quotas** ajudam a isolar *nodes* com GPU e a distribuir cargas de forma justa entre utilizadores ou equipas. Para GPUs com **MIG** (ex: A100), o *device plugin* suporta a exposi√ß√£o de inst√¢ncias individuais (ex: 1g.5gb) como recursos distintos, permitindo agendamento granular e multi-inquilinato seguro. A monitoriza√ß√£o √© feita com ferramentas como **NVIDIA DCGM** integrado com Prometheus/Grafana, fornecendo m√©tricas detalhadas de utiliza√ß√£o por *pod*. Melhores pr√°ticas incluem evitar misturar treino e infer√™ncia no mesmo *node*, usar *PriorityClasses* para cargas cr√≠ticas e considerar *schedulers* avan√ßados (ex: Volcano) para *batch jobs*. Esta abordagem permite gerir clusters de GPU de forma eficiente, resiliente e escal√°vel.

### Laborat√≥rio Pr√°tico: Configurar MIG numa A100

Este laborat√≥rio pr√°tico oferece uma experi√™ncia hands-on para configurar a tecnologia **MIG numa GPU NVIDIA A100**. Os participantes aprender√£o a ativar o modo MIG via `nvidia-smi`, a criar e gerir diferentes **perfis de inst√¢ncia** (ex: 1g.5gb, 2g.10gb, 3g.20gb) que dividem a GPU em parti√ß√µes isoladas, e a atribuir essas inst√¢ncias a cont√™ineres ou cargas de trabalho espec√≠ficas. O exerc√≠cio inclui a verifica√ß√£o da configura√ß√£o com comandos como `nvidia-smi mig -l` e a explora√ß√£o de cen√°rios reais, como a execu√ß√£o paralela de m√∫ltiplos modelos de infer√™ncia ou ambientes de desenvolvimento isolados na mesma GPU f√≠sica. Este laborat√≥rio √© essencial para compreender na pr√°tica como implementar **multi-inquilinato seguro e eficiente**, maximizando o retorno do investimento em hardware de √∫ltima gera√ß√£o e preparando a infraestrutura para ambientes de produ√ß√£o escal√°veis.

### Prerequisitos
- NVIDIA A100 GPU
- Ubuntu 20.04+
- Kubernetes 1.20+
- NVIDIA GPU Driver 465+
- Helm 3+

### Execucao automatica

Execucao completa do script:

```bash
chmod +x scripts/setup-mig.sh
./scripts/setup-mig.sh


## Armazenamento, Redes e Pipelines de Dados para IA

### Arquiteturas de Armazenamento para Cargas de Trabalho de IA (local, compartilhado, objeto)

O armazenamento na infraestrutura de IA √© um componente cr√≠tico de desempenho, pois modelos de grande escala realizam leituras e escritas massivas de dados. Escolher a arquitetura correta ‚Äî desde SSDs locais (NVMe) para velocidade, armazenamento compartilhado (como NFS ou Lustre) para treinamento distribu√≠do, ou armazenamento de objeto (S3, GCS) para escalabilidade e custo ‚Äî impacta diretamente a throughput, lat√™ncia e utiliza√ß√£o da GPU. Sistemas do mundo real frequentemente combinam esses tipos em uma arquitetura h√≠brida e em camadas (hot, warm, cold data) para otimizar custo e performance, garantindo que os GPUs nunca fiquem ociosos esperando por dados.

### Rede de Alta Velocidade: NVLink, Infiniband, RDMA

Em cargas de trabalho de IA distribu√≠da, a rede √© t√£o crucial quanto o poder de computa√ß√£o. Tecnologias como o NVLink da NVIDIA permitem comunica√ß√£o ultrarr√°pida entre GPUs no mesmo n√≥, enquanto o InfiniBand √© o padr√£o-ouro para interconex√£o de alta largura de banda e baixa lat√™ncia entre n√≥s em clusters. O RDMA (Remote Direct Memory Access) √© fundamental, permitindo a transfer√™ncia direta de dados entre a mem√≥ria de m√°quinas diferentes, contornando a CPU e reduzindo drasticamente a lat√™ncia e a sobrecarga. A combina√ß√£o dessas tecnologias, juntamente com features como GPUDirect, √© essencial para opera√ß√µes como "all-reduce" durante o treinamento distribu√≠do de modelos grandes, como GPT ou BERT, garantindo que a sincroniza√ß√£o de gradientes n√£o se torne um gargalo.

### Gargalos e Otimiza√ß√£o no Movimento de Dados

Um gargalo no movimento de dados ‚Äî seja em E/S de disco, na rede ou no pr√©-processamento ‚Äî pode deixar GPUs caros ociosas, aumentando o tempo de treinamento e custos operacionais. Identificar esses pontos √© o primeiro passo, utilizando ferramentas como \texttt{iostat}, \texttt{nvtop} ou profilers de framework (TensorFlow, PyTorch). A otimiza√ß√£o envolve estrat√©gias como a ado√ß√£o de NVMe, uso de carregamento de dados multi-thread (ex: \texttt{num\_workers} no PyTorch), prefetching, caching de dados localmente e paraleliza√ß√£o do pr√©-processamento (ex: com NVIDIA DALI). Em n√≠vel de rede, tuning de configura√ß√µes (MTU, buffers) e a ado√ß√£o de InfiniBand com RDMA s√£o chave para um fluxo de dados cont√≠nuo e eficiente do storage at√© a GPU.

### Design de Pipeline de Dados para IA (ETL + Treinamento + Infer√™ncia)

Um pipeline de dados de IA bem projetado √© um sistema interconectado que abrange desde a ingest√£o de dados brutos (ETL) at√© o treinamento e a infer√™ncia. O est√°gio de ETL, frequentemente orquestrado por ferramentas como Apache Airflow e acelerado por GPUs (RAPIDS, DALI), √© respons√°vel por extrair, transformar e carregar dados em um storage acess√≠vel. No treinamento, o pipeline deve alimentar os GPUs de forma cont√≠nua, usando data loaders paralelizados e formatos eficientes. Para infer√™ncia, em lote ou tempo real, servidores de modelo como o Triton Inference Server s√£o utilizados para oferecer baixa lat√™ncia e alto throughput. Projetar com resili√™ncia, monitoramento e est√°gios desacoplados (usando filas como Kafka) garante um pipeline robusto e escal√°vel. 

### Laborat√≥rio: Projetar um Pipeline de Dados de Ponta a Ponta para IA

Neste laborat√≥rio pr√°tico, consolidamos todos os conceitos anteriores para projetar e implementar um pipeline completo. Isso envolve a configura√ß√£o de uma arquitetura de armazenamento em camadas (ex: S3 para dados brutos, BeeGFS/Lustre para datasets de treinamento), a configura√ß√£o de rede de alta velocidade (InfiniBand com RDMA) e a constru√ß√£o do fluxo de dados em si. Voc√™ poder√° orquestrar um pipeline que ingere dados de um stream em tempo real (Kafka), realiza ETL acelerada, treina um modelo em um cluster de GPUs interconectados com NVLink/InfiniBand e, finalmente, implanta o modelo para infer√™ncia em um ambiente escal√°vel como Kubernetes, utilizando otimiza√ß√µes para evitar gargalos e garantir a m√°xima utiliza√ß√£o dos recursos.

Pipeline completo de dados para AI conectando ETL ‚Üí Model Training ‚Üí Inference usando componentes acelerados por GPU.

Pipeline completo de AI que demonstra:
- ETL com NVIDIA DALI
- Treinamento de modelo com PyTorch
- Deploy com Triton Inference Server
- Monitoramento de performance

üõ†Ô∏è Tecnologias
- **Python** (PyTorch)
- **NVIDIA DALI** (pr√©-processamento acelerado)
- **Docker** & **Docker Compose**
- **Triton Inference Server**
- **Jupyter Notebook**

### Pr√©-requisitos
- NVIDIA GPU com drivers atualizados
- Docker e NVIDIA Container Toolkit
- Python 3.8+

### 1. Configura√ß√£o do Ambiente
```bash
# Clone o reposit√≥rio (se aplic√°vel)
cd end-to-end-pipeline

# Configura ambiente
chmod +x scripts/setup_environment.sh
./scripts/setup_environment.sh

# Op√ß√£o 1: Executar Usando script
chmod +x scripts/run_pipeline.sh
./scripts/run_pipeline.sh

# Op√ß√£o 2: Executar Diretamente com Python
python run_pipeline.py

# Inicia Triton Inference Server
chmod +x scripts/start_triton.sh
./scripts/start_triton.sh

# Testar Inferencia
python -c "from src.inference.triton_client import test_triton_connection; test_triton_connection()"


## Orquestra√ß√£o e Escalabilidade de Clusters de IA

### Kubernetes para Cargas de Trabalho de IA com GPU
O Kubernetes tornou-se a plataforma fundamental para orquestrar cargas de trabalho de IA em produ√ß√£o, especialmente quando envolvem GPUs. Atrav√©s do plugin de dispositivo NVIDIA, o Kubernetes pode reconhecer e alocar GPUs nos n√≥s do cluster, permitindo que jobs de treinamento e servi√ßos de inference sejam escalados de forma eficiente. Na ind√∫stria, bancos utilizam Kubernetes para isolar jobs de treinamento de modelos de fraude enquanto mant√™m servi√ßos de inference de baixa lat√™ncia, tudo no mesmo cluster. Empresas de healthcare usam namespaces e quotas de recursos para segregar workloads de diferentes projetos de pesquisa, garantindo conformidade com regulamenta√ß√µes enquanto maximizam a utiliza√ß√£o dos recursos de GPU.

### Helm, Operators e Autoscaling de Cluster
Helm funciona como um gerenciador de pacotes para Kubernetes, permitindo implantar stacks completos de IA como Kubeflow ou Triton Inference Server com um √∫nico comando. Operators trazem intelig√™ncia espec√≠fica de dom√≠nio, automatizando opera√ß√µes complexas como scaling de pods do Triton baseado no tr√°fego de inference. No varejo, empresas usam HPA (Horizontal Pod Autoscaler) baseado em m√©tricas customizadas de utiliza√ß√£o de GPU para dimensionar automaticamente servi√ßos de recomenda√ß√£o de produtos durante picos de tr√°fego. O Cluster Autoscaler adiciona n√≥s GPU sob demanda para treinamento sazonal e os remove para economizar custos, uma pr√°tica comum em e-commerce durante per√≠odos promocionais.

### Integra√ß√£o de Slurm, Kubeflow e MLflow
A integra√ß√£o dessas ferramentas cria um ambiente completo de MLOps que atende diferentes personas: pesquisadores HPC, cientistas de dados e engenheiros de ML. Slurm oferece escalonamento eficiente para jobs batch de grande escala, comum em institui√ß√µes financeiras para simula√ß√µes de risco. Kubeflow automatiza pipelines de retreinamento de modelos, usado por hospitais para atualizar modelos de diagn√≥stico baseados em novos exames. MLflow fornece rastreabilidade completa, essencial em ind√∫strias regulamentadas onde cada vers√£o de modelo deve ser audit√°vel. Universidades frequentemente combinam Slurm para pesquisa tradicional com Kubeflow para projetos de ML, compartilhando o mesmo cluster de GPUs.

### Topologias de Cluster (On-prem, Cloud, H√≠brido)
A escolha da topologia impacta diretamente custo, desempenho e conformidade. Clusters on-prem, como os baseados em DGX SuperPOD, s√£o preferidos por institui√ß√µes financeiras e de sa√∫de para dados sens√≠veis, oferecendo controle total e baixa lat√™ncia. Cloud nativo √© ideal para startups e projetos experimentais, permitindo escalar rapidamente com inst√¢ncias GPU especializadas. O modelo h√≠brido √© predominante em empresas estabelecidas: fabricantes mant√™m treinamento on-prem para proteger IP, mas usam cloud para inference global. Empresas de energia usam hybrid para processar dados de sensores no edge enquanto consolidam an√°lises na cloud.

### Laborat√≥rio: Deploy de Job de Treinamento Multi-GPU no Kubernetes
Este laborat√≥rio pr√°tico demonstra como implantar jobs distribu√≠dos de treinamento em clusters Kubernetes com m√∫ltiplas GPUs. Atrav√©s de manifests YAML e usando recursos como NodeSelectors e Tolerations, √© poss√≠vel direcionar jobs para n√≥s espec√≠ficos com GPUs dispon√≠veis. Empresas de tecnologia implementam este padr√£o para treinar modelos de linguagem grande distribu√≠dos across m√∫ltiplos n√≥s GPU, enquanto servi√ßos de streaming usam abordagem similar para treinar modelos de recomenda√ß√£o em escala. O laborat√≥rio tamb√©m cobre monitoramento com Prometheus para otimizar utiliza√ß√£o de recursos, pr√°tica adotada por operadores de data center para maximizar ROI em infraestrutura GPU.

#### üéØ Objetivo
Executar treinamento distribu√≠do PyTorch DDP em Kubernetes com:
- Single-node multi-GPU
- Multi-node multi-GPU

#### üìã Pr√©-requisitos
- Cluster Kubernetes (v1.24+)
- NVIDIA GPU drivers + nvidia-container-toolkit
- NVIDIA K8s device plugin
- kubectl, helm
- M√≠nimo 1 n√≥ GPU (single-node) ou 2 n√≥s GPU (multi-node)

## üöÄ Quick Start

### 1. Setup do Cluster
```bash
./scripts/setup-cluster.sh

## 2. Single-node Multi-GPU
./scripts/deploy-single-node.sh

## 3. Multi-node Multi-GPU
./scripts/deploy-multi-node.sh

## 4. Monitoramento
./scripts/monitor-job.sh

```

## Otimiza√ß√£o de Desempenho e Monitoramento

### Profiling de Workloads em GPU (Nsight, DLProf, nvtop)**

O profiling de workloads em GPU √© fundamental para identificar gargalos de performance que impedem o aproveitamento m√°ximo do hardware. O **Nsight Systems** fornece uma vis√£o macro da intera√ß√£o entre CPU e GPU, permitindo identificar tempos ociosos, problemas de sincroniza√ß√£o e sobreposi√ß√£o de transfer√™ncias de dados. J√° o **Nsight Compute** oferece an√°lise granular de kernels CUDA, revelando m√©tricas cr√≠ticas como ocupa√ß√£o, throughput de instru√ß√µes e efici√™ncia de warps. Para workloads espec√≠ficos de deep learning, o **DLProf** realiza profiling camada por camada, detectando se opera√ß√µes est√£o utilizando tensor cores adequadamente ou executando em precis√µes n√£o otimizadas. Complementarmente, o **nvtop** serve como ferramenta de monitoramento em tempo real via terminal, ideal para verifica√ß√£o r√°pida de utiliza√ß√£o em ambientes multi-GPU.

### M√©tricas de GPU, Telemetria e Ferramentas de Alertas**

A telemetria cont√≠nua de GPUs √© essencial para opera√ß√µes em produ√ß√£o. M√©tricas cr√≠ticas incluem utiliza√ß√£o de Streaming Multiprocessors, consumo de mem√≥ria, temperatura, consumo energ√©tico e taxas de erro ECC. O **NVIDIA SMI** fornece snapshots b√°sicos, enquanto o **Data Center GPU Manager (DCGM)** oferece monitoramento em escala com integra√ß√£o nativa ao **Prometheus** para armazenamento de s√©ries temporais. Esta telemetria permite a cria√ß√£o de dashboards no **Grafana** para visualiza√ß√£o de tend√™ncias e configura√ß√£o de alertas proativos para condi√ß√µes como superaquecimento, subutiliza√ß√£o ou degrada√ß√£o de hardware, podendo ser integrados a sistemas de resposta a incidentes como PagerDuty.

### TensorRT e Otimiza√ß√£o de Modelos**

O **TensorRT** √© o SDK especializado da NVIDIA para otimiza√ß√£o de infer√™ncia, transformando modelos treinados em motores de execu√ß√£o altamente eficientes. Suas t√©cnicas de otimiza√ß√£o incluem **layer fusion** (combina√ß√£o de opera√ß√µes em kernels √∫nicos), **mixed precision inference** (execu√ß√£o em FP16/INT8 com calibra√ß√£o para manter acur√°cia), **dynamic tensor memory** (gerenciamento eficiente de mem√≥ria) e **kernel autotuning** (sele√ß√£o autom√°tica dos melhores kernels para cada GPU). Estas otimiza√ß√µes tipicamente resultam em ganhos de 4-6x em throughput e redu√ß√£o de lat√™ncia, sendo particularmente valiosas em aplica√ß√µes onde tempo de resposta √© cr√≠tico.

### Diagn√≥stico e Ajuste de Gargalos**

O diagn√≥stico sistem√°tico de gargalos requer an√°lise hol√≠stica de toda a stack de AI. Gargalos comuns incluem: **coordena√ß√£o CPU-GPU** (GPU ociosa esperando por dados), **utiliza√ß√£o sub√≥tima de GPU** (kernels ineficientes ou batch sizes pequenos), **limita√ß√µes de banda de mem√≥ria**, **IO lento** em pipelines de dados e **satura√ß√£o de rede** em treinamento distribu√≠do. Ferramentas como Nsight Systems e m√©tricas do NVIDIA SMI permitem identificar estes pontos de estrangulamento, enquanto estrat√©gias de tuning incluem ajuste de batch size, precis√£o mista, sobreposi√ß√£o de computa√ß√£o e comunica√ß√£o, memory pinning, RDMA e otimiza√ß√£o de par√¢metros de lan√ßamento de kernels.

### Laboratorio: Otimiza√ß√£o de Pipeline de Infer√™ncia com TensorRT**

Este laborat√≥rio pr√°tico guia na otimiza√ß√£o de um modelo PyTorch de vis√£o computacional, estabelecendo primeiro uma baseline em FP32 e subsequentemente aplicando otimiza√ß√µes do TensorRT em FP16 e potencialmente INT8. A integra√ß√£o com **Triton Inference Server** permite explorar otimiza√ß√µes do lado do servidor como **dynamic batching** (agrupamento din√¢mico de requisi√ß√µes) e **multiple model instances** (m√∫ltiplas inst√¢ncias para paralelismo). As m√©tricas de lat√™ncia e throughput s√£o medidas em cada etapa, demonstrando o impacto tang√≠vel das otimiza√ß√µes em cen√°rios reais de infer√™ncia.

### üéØ Objetivo
Otimizar um modelo de vis√£o computacional PyTorch usando TensorRT, comparando desempenho entre:
- Baseline FP32 (ONNX)
- TensorRT FP16 
- TensorRT INT8 (opcional)

### üìã Pr√©-requisitos
- 1√ó NVIDIA GPU (A100/RTX/etc.)
- Linux (Ubuntu 20.04+)
- Docker + NVIDIA Container Toolkit
- ~10GB de espa√ßo em disco

### üöÄ Quick Start

#### 1. Configurar Ambiente
```bash
./scripts/setup-environment.sh

# Exportar ONNX + construir engines + benchmark
./scripts/run-complete-pipeline.sh

# EWxecucao manual
# 1. Exportar modelo para ONNX
./scripts/export-onnx.sh

# 2. Construir engines TensorRT
./scripts/build-tensorrt-engines.sh

# 3. Iniciar servidor Triton
./scripts/start-triton-server.sh

# 4. Executar benchmarks
./scripts/run-benchmarks.sh

# 5. Validar corre√ß√£o
./scripts/validate-correctness.sh
```

### O Que Esperar
1. Baseline FP32: ~100-200 ms de lat√™ncia
2. TensorRT FP16: 2-3x speedup vs FP32
3. TensorRT INT8: 3-4x speedup vs FP32 (com pequena perda de precis√£o)
4. Dynamic Batching: Melhora throughput em 2-5x
5. Multi-instance: Melhor utiliza√ß√£o da GPU

## Seguran√ßa, Conformidade e Governan√ßa de Dados

### Protegendo Workloads Acelerados por GPU**
A seguran√ßa de cargas de trabalho aceleradas por GPU apresenta desafios √∫nicos em infraestruturas de IA, especialmente em ambientes multi-inquilino onde dados sens√≠veis, como registros m√©dicos, financeiros ou propriet√°rios, s√£o processados. A NVIDIA aborda essas amea√ßas atrav√©s de m√∫ltiplas camadas de seguran√ßa integradas diretamente no *hardware*, incluindo *Secure Boot* para integridade do *firmware*, particionamento de mem√≥ria via MIG e prote√ß√µes ECC, estabelecendo uma base confi√°vel para IA segura. Nos n√≠veis de *software* e cluster, a seguran√ßa se estende por meio de *toolkits* de cont√™ineres verificados, monitoramento cont√≠nuo e pol√≠ticas do Kubernetes, criando uma estrat√©gia de defesa em profundidade essencial para proteger sistemas de IA em escala.

### Criptografia e Controle de Acesso (DPUs, DOCA)**
A criptografia e o controle de acesso formam a base da seguran√ßa de dados em infraestruturas de IA, protegendo informa√ß√µes sens√≠veis em repouso e em tr√¢nsito. Em clusters de GPU compartilhados, esses controles previnem a exposi√ß√£o cruzada entre inquilinos. Os DPUs (*Data Processing Units*) da NVIDIA revolucionam essa abordagem ao descarregar fun√ß√µes de seguran√ßa diretamente no *hardware* atrav√©s da arquitetura DOCA. Essas unidades atuam como *gatekeepers* de confian√ßa zero na borda do cluster, aplicando firewalls, criptografia e inspe√ß√£o de pacotes em linha, sem impactar o desempenho das GPUs, permitindo uma seguran√ßa aplicada de forma transparente abaixo da camada de aplica√ß√£o.

### Controle de Acesso Baseado em Fun√ß√£o (RBAC) para Clusters de IA**
Em clusters de IA multi-inquilino, o RBAC fornece o mecanismo fundamental para governan√ßa de acesso, definindo permiss√µes com base em fun√ß√µes, e n√£o em indiv√≠duos. Isso √© cr√≠tico quando diversos profissionais compartilham recursos de GPU. O RBAC no Kubernetes opera por meio de quatro componentes principais: *Roles*, *RoleBindings*, *ClusterRoles* e *ClusterRoleBindings*, criando um sistema modular que escala eficientemente. Sua efic√°cia depende da integra√ß√£o com sistemas corporativos de identidade e da aplica√ß√£o do princ√≠pio do privil√©gio m√≠nimo. Quando combinado com tecnologias como DPUs, o RBAC forma o motor de pol√≠ticas para uma infraestrutura de confian√ßa zero.

### Conformidade Regulat√≥ria: GDPR, HIPAA, FedRAMP**
A conformidade regulat√≥ria √© uma obriga√ß√£o legal e um habilitador de neg√≥cios para infraestruturas de IA. O GDPR se aplica a dados de cidad√£os europeus, exigindo consentimento expl√≠cito e direitos de acesso/exclus√£o. O HIPAA rege dados de sa√∫de nos EUA, demandando criptografia e logs de auditoria para Informa√ß√µes de Sa√∫de Protegidas (PHI). O FedRAMP padroniza a autoriza√ß√£o de servi√ßos em nuvem para o governo dos EUA, exigindo monitoramento cont√≠nuo. *Workloads* de IA apresentam desafios √∫nicos de conformidade, e mant√™-la requer uma combina√ß√£o de controles t√©cnicos e processos organizacionais, incorporando a conformidade desde o projeto da infraestrutura.

### Laborat√≥rio: Aplicar Pol√≠ticas de Seguran√ßa em Infraestrutura de IA**
Este laborat√≥rio pr√°tico concentra-se na prote√ß√£o de ambientes Kubernetes habilitados para GPU por meio da aplica√ß√£o de controles de seguran√ßa em camadas. Os participantes implementar√£o pol√≠ticas RBAC, configura√ß√µes de seguran√ßa de *pods*, segmenta√ß√£o de rede, gerenciamento de *secrets* e TLS, al√©m de governan√ßa de recursos para GPUs. O laborat√≥rio inclui a configura√ß√£o de pol√≠ticas de admiss√£o e telemetria b√°sica com alertas. Cada controle de seguran√ßa √© validado por testes pr√°ticos, consolidando os conceitos te√≥ricos e mostrando como combinar ferramentas da NVIDIA com controles nativos do Kubernetes para criar ambientes de IA seguros e prontos para produ√ß√£o.


Proteger um ambiente Kubernetes habilitado para GPU aplicando controles em camadas:

- **RBAC** (Controle de Acesso Baseado em Fun√ß√£o)
- **Seguran√ßa de Pods**
- **Segmenta√ß√£o de Rede**
- **Secrets & TLS**
- **Governan√ßa de Recursos para GPUs**
- **Pol√≠ticas de Admission Control**
- **Telemetria B√°sica + Alertas**

### üìã Pr√©-requisitos
- Cluster Kubernetes (v1.25+)
- kubectl e acesso cluster-admin
- Pelo menos 1 n√≥ com GPU + NVIDIA device plugin
- CNI que suporte NetworkPolicy (Calico/Cilium)
- Opcional: Gatekeeper (OPA) e stack Prometheus/Grafana

### üöÄ Implementa√ß√£o R√°pida

```bash
# Executar implanta√ß√£o completa
./scripts/deploy-all.sh

# Validar controles de seguran√ßa
./scripts/validate-controls.sh

# Limpar ambiente
./scripts/cleanup.sh

```

### Namespaces Seguros
- team-a: Time de Data Science
- team-b: Time de Engenharia

### Controles Implementados
- Pod Security Standards (perfil restrito)
- RBAC com princ√≠pio do menor privil√©gio
- Quotas de GPU e limites de recursos
- NetworkPolicies (default-deny + allow-list)
- Seguran√ßa de Workloads (non-root, read-only FS)
- Admission Control (Gatekeeper)
- Monitoramento de GPU (DCGM Exporter + Alertas)

### Valida√ß√£o
Cada controle √© validado com testes espec√≠ficos para garantir efetividade.

### Manuten√ß√£o
- Atualizar pol√≠ticas conforme mudan√ßas nos requisitos
- Monitorar alertas de seguran√ßa
- Realizar auditorias regulares de RBAC

### How-to

```bash

## Tornar todos os scripts execut√°veis
chmod +x ai-security-lab/scripts/*.sh

cd ai-security-lab

# 1. Implantar tudo
./scripts/deploy-all.sh

# 2. Validar controles
./scripts/validate-controls.sh

# 3. Testar workloads
kubectl get pods -n team-a
kubectl logs -n team-a job/cuda-secure-job

# 4. Limpar (quando necess√°rio)
./scripts/cleanup.sh

```

O Que Foi Implementado
1. Namespaces Seguros com Pod Security Standards
2. RBAC com princ√≠pio do menor privil√©gio
3. Quotas de GPU e limites de recursos
4. NetworkPolicies com default-deny
5. TLS Secrets para comunica√ß√£o segura
6. Workloads Seguros (non-root, read-only FS)
7. Admission Control com Gatekeeper
8. Monitoramento com alertas de seguran√ßa

## Infraestrutura de IA na Edge e Integra√ß√£o

### Edge vs Cloud AI ‚Äì Implica√ß√µes de Infraestrutura**

A escolha entre Edge AI e Cloud AI √© guiada por trade-offs fundamentais em lat√™ncia, banda, seguran√ßa e escalabilidade. O Edge AI processa dados localmente, sendo crucial para aplica√ß√µes em tempo real, como ve√≠culos aut√¥nomos, pois elimina a lat√™ncia do trajeto at√© a nuvem. Al√©m disso, ao manter os dados sens√≠veis no local, o Edge atende a requisitos de privacidade e conformidade regulat√≥ria, como GDPR e HIPAA, e reduz a carga na rede ao transmitir apenas metadados ou insights consolidados. Em contrapartida, a Cloud AI oferece escalabilidade el√°stica quase infinita, permitindo treinar modelos complexos com milhares de GPUs. Na pr√°tica, as organiza√ß√µes adotam estrat√©gias h√≠bridas: o Edge lida com a infer√™ncia em tempo real e a autonomia local, enquanto a nuvem centraliza o treinamento de modelos, an√°lises aprofundadas e a gest√£o do ciclo de vida dos sistemas.

### NVIDIA Jetson e Orin para Edge AI**

As plataformas NVIDIA Jetson e Orin s√£o computadores compactos e energeticamente eficientes projetados para executar IA na ponta. Elas trazem o poder da arquitetura GPU NVIDIA para dispositivos embarcados, permitindo infer√™ncia de alta performance em rob√≥tica, drones, automa√ß√£o industrial e cidades inteligentes. A fam√≠lia Jetson varia do Jetson Nano, para prototipagem, at√© o mais avan√ßado Xavier NX. A gera√ß√£o Orin, baseada na arquitetura Ampere, oferece um desempenho por watt superior, suportando modelos de linguagem natural e vis√£o computacional complexos em tempo real. Essas plataformas s√£o suportadas pelo SDK JetPack e ferramentas como TensorRT e DeepStream, que otimizam a infer√™ncia e permitem a orquestra√ß√£o de frotas de dispositivos, integrando-se perfeitamente em fluxos de trabalho h√≠bridos com a nuvem.

### Aprendizado Federado e Infer√™ncia Distribu√≠da**

O Aprendizado Federado (Federated Learning) √© uma t√©cnica de treinamento colaborativo de modelos de IA em que os dados brutos nunca saem dos dispositivos de edge. Cada dispositivo treina um modelo localmente e envia apenas as atualiza√ß√µes do modelo (n√£o os dados) para um servidor central que agrega essas contribui√ß√µes. Isso preserva a privacidade, atende a regulamenta√ß√µes e reduz o tr√°fego de rede. J√° a Infer√™ncia Distribu√≠da divide a tarefa de executar um modelo de IA entre m√∫ltiplos GPUs ou n√≥s de computa√ß√£o, sendo essencial para modelos grandes e para garantir baixa lat√™ncia e escalabilidade em produ√ß√£o. Juntas, essas t√©cnicas formam um ciclo de feedback: o aprendizado federado melhora o modelo global de forma privada, e a infer√™ncia distribu√≠da serve esse modelo atualizado de forma eficiente na ponta, criando sistemas de IA escal√°veis, seguros e de alto desempenho.

### Casos de Uso: Cidades Inteligentes, Varejo e IIoT**

A Edge AI est√° transformando setores como Cidades Inteligentes, Varejo e IoT Industrial (IIoT). Nas **Cidades Inteligentes**, c√¢meras com IA na ponta analisam v√≠deo em tempo real para gest√£o de tr√°fego e seguran√ßa p√∫blica, enviando apenas metadados para a nuvem, o que garante efici√™ncia e privacidade. No **Varejo**, sistemas de checkout automatizado, recomenda√ß√µes personalizadas em loja e monitoramento de estoque s√£o habilitados por infer√™ncia local, melhorando a experi√™ncia do cliente e a efici√™ncia operacional. No **IIoT**, a IA na ponta viabiliza a manuten√ß√£o preditiva de m√°quinas, a detec√ß√£o de anomalias em tempo real em linhas de produ√ß√£o e a opera√ß√£o segura de rob√¥s colaborativos, aumentando a produtividade e reduzindo custos e tempo de inatividade.

### Laborat√≥rio: Implantar um Modelo de IA no Jetson Nano**

O objetivo deste laborat√≥rio pr√°tico √© implantar um modelo de classifica√ß√£o de imagem em tempo real em uma placa Jetson Nano, utilizando o TensorRT para otimiza√ß√£o nativa de desempenho. Os participantes ir√£o preparar o dispositivo, configurar perfis de energia e t√©rmicos, converter um modelo no formato ONNX para um motor TensorRT e executar a infer√™ncia usando Python. Opcionalmente, o modelo pode ser integrado a um pipeline simples no DeepStream para processamento de v√≠deo. A atividade permite praticar a convers√£o e acelera√ß√£o de modelos, o desenvolvimento de aplica√ß√µes de infer√™ncia na ponta e a medi√ß√£o de m√©tricas de desempenho cr√≠ticas, como FPS (frames por segundo) e lat√™ncia diretamente no dispositivo.

Este laborat√≥rio demonstra a implanta√ß√£o de um modelo de classifica√ß√£o de imagem ResNet50 no Jetson Nano usando TensorRT para infer√™ncia otimizada.

## üìã Pr√©-requisitos

- **Hardware**: Jetson Nano 4GB, fonte 5V 4A, micro-SD 32GB+, cooler
- **Software**: JetPack 4.6+, Python 3.6+

## üöÄ Instala√ß√£o R√°pida

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/jetson-nano-ai-lab.git
cd jetson-nano-ai-lab

# Execute o script de setup
chmod +x scripts/setup_jetson.sh
./scripts/setup_jetson.sh
```

### üîß Configura√ß√£o do Sistema

#### 1. Modo de Alto Desempenho
```bash
sudo nvpmodel -m 0
sudo jetson_clocks
```

#### 2. Configurar Swap
```bash
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
echo '/swapfile swap swap defaults 0 0' | sudo tee -a /etc/fstab
```

### üß† Prepara√ß√£o do Modelo

#### Op√ß√£o A: Exportar do Workstation
```bash
python scripts/export_model.py
scp resnet50.onnx usuario@ip-do-nano:/caminho/do/projeto/
```

#### Op√ß√£o B: Build no Jetson
```bash
chmod +x scripts/build_engine.sh
./scripts/build_engine.sh
```

#### üèÉ‚Äç‚ôÇÔ∏è Execu√ß√£o da Infer√™ncia

```bash
python src/infer_trt.py --engine resnet50_fp16.plan --image caminho/da/imagem.jpg
```

#### üìä DeepStream (Opcional)

```bash
deepstream-app -c configs/ds_resnet.txt
```

#### üìà Performance

- Lat√™ncia esperada: 15-30ms (FP16)
- Throughput: 30-60 FPS

## NGC, Triton Inference Server e Implanta√ß√£o

### Usando o Cat√°logo NGC para Modelos Pr√©-treinados**

O NGC (NVIDIA GPU Cloud) Catalog √© um reposit√≥rio central que oferece containers otimizados para GPU, modelos de IA pr√©-treinados, scripts de fine-tuning e Helm Charts para Kubernetes. Ele acelera significativamente o desenvolvimento de IA, fornecendo recursos como modelos de vis√£o computacional, processamento de linguagem natural e sistemas de recomenda√ß√£o, todos otimizados pela NVIDIA. Para utiliz√°-lo, os desenvolvedores criam uma conta gratuita, geram uma chave de API e podem acessar os assets via portal web, CLI ou comandos Docker. Um fluxo t√≠pico inclui buscar um modelo pr√©-treinado (como ResNet50), fine-tun√°-lo com dados espec√≠ficos e implant√°-lo via Triton Inference Server, assegurando compatibilidade com vers√µes do CUDA e TensorRT para evitar problemas de desempenho.

### Vis√£o Geral e Arquitetura do Triton Inference Server**

O Triton Inference Server √© uma plataforma de c√≥digo aberto para servir modelos de IA em escala, suportando m√∫ltiplos frameworks como TensorFlow, PyTorch, ONNX e TensorRT em um √∫nico servidor. Sua arquitetura inclui um reposit√≥rio de modelos, backends de infer√™ncia espec√≠ficos para cada framework, um agendador inteligente e APIs REST/gRPC. Recursos como execu√ß√£o concorrente de modelos e *dynamic batching* maximizam a utiliza√ß√£o da GPU, agregando requisi√ß√µes para aumentar o throughput. O Triton √© implant√°vel em VMs, containers Docker, Kubernetes e dispositivos de edge como Jetson, sendo ideal para aplica√ß√µes em ve√≠culos aut√¥nomos, sa√∫de e sistemas de recomenda√ß√£o que exigem baixa lat√™ncia e alta escalabilidade.

### Conjunto de Modelos e Servi√ßo Multi-Framework**

Os *model ensembles* do Triton permitem criar pipelines de infer√™ncia encadeando v√°rios modelos, mesmo de frameworks diferentes, em um √∫nico fluxo. Isso elimina a necessidade de chamadas externas entre est√°gios, reduzindo a lat√™ncia e simplificando o gerenciamento. Por exemplo, um pipeline de classifica√ß√£o de imagem pode incluir um modelo de pr√©-processamento em TensorFlow, um modelo de infer√™ncia principal otimizado com TensorRT e um p√≥s-processamento em PyTorch ‚Äî tudo gerenciado internamente pelo Triton. Essa capacidade √© crucial para aplica√ß√µes complexas, como pipelines de √°udio (ASR + NLP) ou sistemas de dire√ß√£o aut√¥noma, que dependem de m√∫ltiplos est√°gios de processamento.

### Servindo em Escala ‚Äì Balanceamento de Carga e Design de Alta Disponibilidade**

Para garantir confiabilidade em produ√ß√£o, √© essencial escalar o Triton horizontalmente com balanceamento de carga e alta disponibilidade (HA). Estrat√©gias como *round-robin* ou *least connections* distribuem as requisi√ß√µes entre m√∫ltiplas inst√¢ncias, enquanto configura√ß√µes ativo-ativo ou ativo-passivo previnem tempos de inatividade. Em Kubernetes, o Horizontal Pod Autoscaler ajusta o n√∫mero de r√©plicas com base na utiliza√ß√£o de GPU, e ferramentas como Prometheus monitoram a sa√∫de dos n√≥s. Projetos h√≠bridos ou multi-region com balanceadores globais (AWS ALB, Cloudflare) asseguram resili√™ncia contra falhas, atendendo a SLAs rigorosos em aplica√ß√µes cr√≠ticas, como cidades inteligentes e ve√≠culos aut√¥nomos.

### Laborat√≥rio: Implantar o Triton com Modelos TensorFlow e ONNX**

Este laborat√≥rio pr√°tico guiar√° os alunos na implanta√ß√£o de dois modelos ‚Äî um do TensorFlow e outro no formato ONNX ‚Äî no Triton Inference Server. Os participantes configurar√£o o reposit√≥rio de modelos, definir√£o os arquivos de configura√ß√£o e iniciar√£o o servidor para servir ambos os modelos simultaneamente. A atividade demonstrar√° a capacidade do Triton de gerenciar m√∫ltiplos frameworks em um √∫nico ambiente, com os alunos enviando requisi√ß√µes de infer√™ncia via gRPC ou HTTP para validar o funcionamento ponta a ponta.

#### Objetivo
Implantar modelos TensorFlow e ONNX no NVIDIA Triton Inference Server e servir atrav√©s de APIs unificadas.

#### Pr√©-requisitos
- NVIDIA GPU com drivers instalados
- Docker e NVIDIA Container Toolkit
- 50GB de espa√ßo livre em disco

#### Configura√ß√£o R√°pida

```bash
# Clone o reposit√≥rio
git clone <seu-repositorio>
cd triton-tensorflow-onnx-lab

# Execute o setup
chmod +x scripts/setup_environment.sh
./scripts/setup_environment.sh

# Baixe os modelos
./scripts/download_models.sh

# Inicie o Triton
./scripts/start_triton.sh

```

## Projetos do Mundo Real e Fluxos de Trabalho Empresariais

## Projeto Final e Prepara√ß√£o para Certifica√ß√£o
