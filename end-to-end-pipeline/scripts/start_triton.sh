#!/bin/bash

echo "üöÄ Starting Triton Inference Server..."

# Verifica se Docker est√° instalado
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker n√£o encontrado. Por favor, instale Docker primeiro."
    exit 1
fi

# Verifica se NVIDIA Container Toolkit est√° instalado
if ! docker run --rm --gpus all nvidia/cuda:11.8-base nvidia-smi &> /dev/null; then
    echo "‚ùå NVIDIA Container Toolkit n√£o configurado corretamente."
    echo "   Por favor, instale: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html"
    exit 1
fi

# Verifica se o reposit√≥rio de modelos existe
if [ ! -d "src/inference/model_repository" ]; then
    echo "‚ùå Model repository n√£o encontrado. Execute o pipeline primeiro:"
    echo "   python run_pipeline.py"
    exit 1
fi

echo "üìÅ Model repository: src/inference/model_repository"
echo "üåê Triton endpoints:"
echo "   - HTTP: localhost:8080"
echo "   - gRPC: localhost:8081"
echo "   - Metrics: localhost:8082"

# Inicia o servidor Triton
docker run --gpus all --rm -p 8080:8080 -p 8081:8081 -p 8082:8082 \
    -v $(pwd)/src/inference/model_repository:/models \
    nvcr.io/nvidia/tritonserver:24.03-py3 \
    tritonserver --model-repository=/models --log-verbose=1