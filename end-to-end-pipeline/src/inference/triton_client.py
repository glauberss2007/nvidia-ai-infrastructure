import numpy as np
import tritonclient.http as httpclient
from tritonclient.utils import triton_to_np_dtype
import time
import os

class TritonClient:
    def __init__(self, url="localhost:8080"):
        self.client = httpclient.InferenceServerClient(url=url)
        
    def check_server_ready(self):
        """Verifica se o servidor Triton estÃ¡ pronto"""
        try:
            return self.client.is_server_ready()
        except Exception as e:
            print(f"âŒ Triton server not ready: {e}")
            return False
    
    def check_model_ready(self, model_name="my_model"):
        """Verifica se o modelo estÃ¡ pronto"""
        try:
            return self.client.is_model_ready(model_name)
        except Exception as e:
            print(f"âŒ Model not ready: {e}")
            return False
    
    def inference(self, model_name, input_data):
        """Executa inferÃªncia no modelo"""
        
        # Prepara a entrada
        inputs = httpclient.InferInput("INPUT__0", input_data.shape, "FP32")
        inputs.set_data_from_numpy(input_data.astype(np.float32))
        
        # Prepara a saÃ­da
        outputs = httpclient.InferRequestedOutput("OUTPUT__0")
        
        # Executa inferÃªncia
        result = self.client.infer(model_name, inputs=[inputs], outputs=[outputs])
        
        return result.as_numpy("OUTPUT__0")
    
    def benchmark_model(self, model_name, input_shape=(1, 3, 32, 32), iterations=100):
        """Benchmark do modelo com dados aleatÃ³rios"""
        print(f"ğŸ§ª Running benchmark for {model_name}...")
        
        total_time = 0
        for i in range(iterations):
            # Gera dados aleatÃ³rios
            input_data = np.random.randn(*input_shape).astype(np.float32)
            
            # Executa inferÃªncia
            start_time = time.time()
            result = self.inference(model_name, input_data)
            end_time = time.time()
            
            total_time += (end_time - start_time)
            
            if i % 20 == 0:
                print(f"   Iteration {i}: {result.shape} - {end_time - start_time:.4f}s")
        
        avg_time = total_time / iterations
        print(f"ğŸ“Š Average inference time: {avg_time:.4f}s")
        print(f"ğŸ“Š Throughput: {1/avg_time:.2f} inferences/second")
        
        return avg_time

def create_model_repository():
    """Cria a estrutura do repositÃ³rio de modelos do Triton"""
    import os
    
    # Estrutura de diretÃ³rios
    directories = [
        "src/inference/model_repository/my_model/1",
        "src/inference/model_repository/my_model"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    
    print("âœ… Model repository structure created")

def test_triton_connection():
    """Testa a conexÃ£o com o Triton Server"""
    client = TritonClient()
    
    if client.check_server_ready():
        print("âœ… Triton server is ready!")
        
        if client.check_model_ready("my_model"):
            print("âœ… Model 'my_model' is ready!")
            
            # Testa inferÃªncia
            test_input = np.random.randn(1, 3, 32, 32).astype(np.float32)
            result = client.inference("my_model", test_input)
            print(f"âœ… Test inference successful! Output shape: {result.shape}")
            
            return True
        else:
            print("âŒ Model 'my_model' is not ready")
            return False
    else:
        print("âŒ Triton server is not ready")
        return False