apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ddp
  namespace: ai-lab
  labels:
    app: ddp
    type: multi-node-training
spec:
  serviceName: ddp-hs
  replicas: 2
  selector:
    matchLabels:
      app: ddp
  template:
    metadata:
      labels:
        app: ddp
    spec:
      restartPolicy: Always
      containers:
      - name: trainer
        image: nvcr.io/nvidia/pytorch:24.03-py3
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            nvidia.com/gpu: 2
          requests:
            nvidia.com/gpu: 2
        ports:
        - containerPort: 29400
          name: rdzv
        env:
        - name: NCCL_ASYNC_ERROR_HANDLING
          value: "1"
        - name: NCCL_IB_DISABLE
          value: "1"
        - name: OMP_NUM_THREADS
          value: "4"
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - name: script
          mountPath: /workspace/train.py
          subPath: train.py
        - name: outputs
          mountPath: /outputs
        command: ["bash", "-lc"]
        args:
        - |
          set -e
          # Derive this pod's ordinal from HOSTNAME (ddp-0, ddp-1, ...)
          ORD=${HOSTNAME##*-}
          MASTER_ADDR="ddp-0.ddp-hs.ai-lab.svc.cluster.local"
          MASTER_PORT=29400
          NNODES=2
          NPROC_PER_NODE=2
          NODE_RANK=$ORD

          echo "ðŸ”§ Node configuration:"
          echo "   HOSTNAME: $HOSTNAME"
          echo "   ORD: $ORD"
          echo "   MASTER_ADDR: $MASTER_ADDR"
          echo "   NODE_RANK: $NODE_RANK"

          cd /workspace && \
          torchrun \
            --nnodes=$NNODES \
            --nproc_per_node=$NPROC_PER_NODE \
            --node_rank=$NODE_RANK \
            --master_addr=$MASTER_ADDR \
            --master_port=$MASTER_PORT \
            /workspace/train.py --epochs 2 --batch-size 256 --out-dir /outputs
      volumes:
      - name: script
        configMap:
          name: ddp-train
      - name: outputs
        emptyDir: {}