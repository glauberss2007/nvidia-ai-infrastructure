---
# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: ai-lab
  labels:
    name: ai-lab
    purpose: distributed-training

---
# Training Script ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: ddp-train
  namespace: ai-lab
data:
  train.py: |
    import os
    import argparse
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.distributed as dist
    from torch.nn.parallel import DistributedDataParallel as DDP
    from torchvision import datasets, transforms, models
    from torch.utils.data import DataLoader, DistributedSampler

    def parse_args():
        p = argparse.ArgumentParser()
        p.add_argument("--epochs", type=int, default=2)
        p.add_argument("--batch-size", type=int, default=256)
        p.add_argument("--data-dir", type=str, default="/workspace/data")
        p.add_argument("--out-dir", type=str, default="/outputs")
        return p.parse_args()

    def setup_distributed():
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend="nccl")
        return local_rank

    def main():
        args = parse_args()
        os.makedirs(args.out_dir, exist_ok=True)
        
        local_rank = setup_distributed()
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        
        if rank == 0:
            print(f"ðŸš€ Starting DDP training with {world_size} processes")

        transform_train = transforms.Compose([
            transforms.Resize(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        
        dataset = datasets.CIFAR10(
            root=args.data_dir, 
            train=True, 
            download=True,
            transform=transform_train
        )
        
        sampler = DistributedSampler(
            dataset, 
            num_replicas=world_size, 
            rank=rank,
            shuffle=True
        )
        
        loader = DataLoader(
            dataset, 
            batch_size=args.batch_size, 
            sampler=sampler,
            num_workers=4,
            pin_memory=True
        )
        
        model = models.resnet18(num_classes=10).cuda()
        model = DDP(model, device_ids=[local_rank])
        
        criterion = nn.CrossEntropyLoss().cuda()
        optimizer = optim.Adam(model.parameters(), lr=1e-3)
        
        for epoch in range(args.epochs):
            sampler.set_epoch(epoch)
            model.train()
            running_loss = 0.0
            
            for i, (x, y) in enumerate(loader):
                x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)
                
                optimizer.zero_grad(set_to_none=True)
                out = model(x)
                loss = criterion(out, y)
                loss.backward()
                optimizer.step()
                
                running_loss += loss.item()
                
                if i % 50 == 0 and rank == 0:
                    avg_loss = running_loss / (i + 1)
                    print(f"[epoch {epoch}] step {i} avg_loss={avg_loss:.4f}")
            
            if rank == 0:
                checkpoint_path = os.path.join(args.out_dir, f"resnet18_epoch_{epoch}.pth")
                torch.save(model.module.state_dict(), checkpoint_path)
                print(f"ðŸ’¾ Saved checkpoint: {checkpoint_path}")
        
        if rank == 0:
            print("âœ… Training completed successfully!")
        
        dist.destroy_process_group()

    if __name__ == "__main__":
        main()

---
# Single-Node Multi-GPU Job
apiVersion: batch/v1
kind: Job
metadata:
  name: ddp-1node
  namespace: ai-lab
  labels:
    app: ddp-training
    type: single-node
spec:
  backoffLimit: 0
  template:
    metadata:
      labels:
        job-name: ddp-1node
    spec:
      restartPolicy: Never
      containers:
      - name: trainer
        image: nvcr.io/nvidia/pytorch:24.03-py3
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            nvidia.com/gpu: 4
          requests:
            nvidia.com/gpu: 4
        env:
        - name: NCCL_ASYNC_ERROR_HANDLING
          value: "1"
        - name: NCCL_IB_DISABLE
          value: "1"
        - name: OMP_NUM_THREADS
          value: "4"
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - name: script
          mountPath: /workspace/train.py
          subPath: train.py
        - name: outputs
          mountPath: /outputs
        command: ["bash", "-lc"]
        args:
        - |
          cd /workspace && \
          torchrun --standalone --nproc_per_node=4 \
            /workspace/train.py --epochs 2 --batch-size 256 --out-dir /outputs
      volumes:
      - name: script
        configMap:
          name: ddp-train
      - name: outputs
        emptyDir: {}

---
# Multi-Node: Headless Service
apiVersion: v1
kind: Service
metadata:
  name: ddp-hs
  namespace: ai-lab
  labels:
    app: ddp
    type: headless-service
spec:
  clusterIP: None
  selector:
    app: ddp
  ports:
  - name: rdzv
    port: 29400
    targetPort: 29400

---
# Multi-Node: StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ddp
  namespace: ai-lab
  labels:
    app: ddp
    type: multi-node-training
spec:
  serviceName: ddp-hs
  replicas: 2
  selector:
    matchLabels:
      app: ddp
  template:
    metadata:
      labels:
        app: ddp
    spec:
      restartPolicy: Always
      containers:
      - name: trainer
        image: nvcr.io/nvidia/pytorch:24.03-py3
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            nvidia.com/gpu: 2
          requests:
            nvidia.com/gpu: 2
        ports:
        - containerPort: 29400
          name: rdzv
        env:
        - name: NCCL_ASYNC_ERROR_HANDLING
          value: "1"
        - name: NCCL_IB_DISABLE
          value: "1"
        - name: OMP_NUM_THREADS
          value: "4"
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - name: script
          mountPath: /workspace/train.py
          subPath: train.py
        - name: outputs
          mountPath: /outputs
        command: ["bash", "-lc"]
        args:
        - |
          set -e
          ORD=${HOSTNAME##*-}
          MASTER_ADDR="ddp-0.ddp-hs.ai-lab.svc.cluster.local"
          MASTER_PORT=29400
          NNODES=2
          NPROC_PER_NODE=2
          NODE_RANK=$ORD

          echo "ðŸ”§ Node configuration:"
          echo "   HOSTNAME: $HOSTNAME"
          echo "   ORD: $ORD"
          echo "   MASTER_ADDR: $MASTER_ADDR"
          echo "   NODE_RANK: $NODE_RANK"

          cd /workspace && \
          torchrun \
            --nnodes=$NNODES \
            --nproc_per_node=$NPROC_PER_NODE \
            --node_rank=$NODE_RANK \
            --master_addr=$MASTER_ADDR \
            --master_port=$MASTER_PORT \
            /workspace/train.py --epochs 2 --batch-size 256 --out-dir /outputs
      volumes:
      - name: script
        configMap:
          name: ddp-train
      - name: outputs
        emptyDir: {}

---
# Persistent Volume Claim (Optional)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ddp-outputs
  namespace: ai-lab
  labels:
    app: ddp-training
    type: storage
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard