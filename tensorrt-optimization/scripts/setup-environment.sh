#!/bin/bash

set -e

echo "ğŸš€ Configurando ambiente para Lab 5 - TensorRT Optimization..."

# Cores para output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m'

# Verificar Docker
echo -e "${BLUE}ğŸ” Verificando Docker...${NC}"
if ! command -v docker &> /dev/null; then
    echo -e "${YELLOW}âŒ Docker nÃ£o encontrado. Por favor instale Docker primeiro.${NC}"
    exit 1
fi

# Verificar NVIDIA Container Toolkit
echo -e "${BLUE}ğŸ” Verificando NVIDIA Container Toolkit...${NC}"
if ! docker run --rm --gpus all nvidia/cuda:11.8-base nvidia-smi &> /dev/null; then
    echo -e "${YELLOW}âŒ NVIDIA Container Toolkit nÃ£o configurado corretamente.${NC}"
    exit 1
fi

# Criar diretÃ³rios
echo -e "${BLUE}ğŸ“ Criando estrutura de diretÃ³rios...${NC}"
mkdir -p ../models
mkdir -p ../calibration
mkdir -p ../benchmarks
mkdir -p ../model_repository

# Pull de containers necessÃ¡rios
echo -e "${BLUE}ğŸ“¦ Baixando containers NVIDIA...${NC}"
docker pull nvcr.io/nvidia/pytorch:24.03-py3
docker pull nvcr.io/nvidia/tritonserver:24.03-py3

echo -e "${GREEN}âœ… Ambiente configurado com sucesso!${NC}"
echo ""
echo -e "${YELLOW}ğŸ“ PrÃ³ximos passos:${NC}"
echo -e "   ./scripts/export-onnx.sh    # Exportar modelo para ONNX"
echo -e "   ./scripts/build-tensorrt-engines.sh # Construir engines TensorRT"